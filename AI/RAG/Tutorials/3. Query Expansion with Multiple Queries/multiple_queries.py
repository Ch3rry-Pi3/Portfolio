"""
Query Expansion with Multiple Queries

This script demonstrates a retrieval-augmented generation (RAG) pipeline where an original query is expanded using related 
questions generated by an LLM. The expanded set of queries is used to retrieve relevant documents from a vector store, 
followed by result visualisation in a 2D embedding space using UMAP.

The process includes:
1. Loading and preprocessing a PDF document.
2. Splitting text into manageable chunks using advanced text-splitting techniques.
3. Generating embeddings for text chunks and storing them in a ChromaDB vector store.
4. Expanding the query by generating related questions with an LLM.
5. Querying the vector store with the expanded queries.
6. Visualising query embeddings and retrieved document embeddings in a 2D space using UMAP.

"""

# ==========================
# 1. Import Libraries
# ==========================
import os
import umap
import matplotlib.pyplot as plt
import numpy as np
from pypdf import PdfReader
from dotenv import load_dotenv
from openai import OpenAI
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    SentenceTransformersTokenTextSplitter,
)
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from helper_utils import project_embeddings, word_wrap
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

# ==========================
# 2. Configuration Setup
# ==========================
# Load environment variables from .env file
load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")  # Retrieve OpenAI API key from .env file
client = OpenAI(api_key=openai_key)  # Initialise OpenAI client

# ==========================
# 3. Load and Preprocess PDF
# ==========================
pdf_path = "data/microsoft-annual-report.pdf"  # Path to the PDF document
reader = PdfReader(pdf_path)  # Read the PDF file
pdf_texts = [p.extract_text().strip() for p in reader.pages if p.extract_text()]  # Extract text from each page

# ==========================
# 4. Text Splitting
# ==========================
# Character-based splitting
character_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". ", " ", ""], chunk_size=1000, chunk_overlap=0
)  # Define splitting strategy based on character chunks
character_split_texts = character_splitter.split_text("\n\n".join(pdf_texts))  # Split extracted text

# Token-based splitting
token_splitter = SentenceTransformersTokenTextSplitter(
    chunk_overlap=0, tokens_per_chunk=256
)  # Define token-based splitting strategy
token_split_texts = []
for text in character_split_texts:
    token_split_texts += token_splitter.split_text(text)  # Further split using token-based strategy

# ==========================
# 5. ChromaDB Setup and Embedding Generation
# ==========================
# Initialise embedding function and vector store
embedding_function = SentenceTransformerEmbeddingFunction()  # Set up embedding function
chroma_client = chromadb.Client()  # Initialise ChromaDB client
chroma_collection = chroma_client.create_collection(
    "microsoft-collection", embedding_function=embedding_function
)  # Create a collection in the vector store

# Generate and add embeddings to the vector store
ids = [str(i) for i in range(len(token_split_texts))]  # Create unique IDs for each chunk
chroma_collection.add(ids=ids, documents=token_split_texts)  # Add documents and embeddings to the collection

# ==========================
# 6. Query Expansion with LLM
# ==========================
def generate_multi_query(query, model="gpt-3.5-turbo"):
    """
    Expands the query by generating multiple related questions using an LLM.

    Args:
        query (str): The original query.
        model (str): The LLM model to use.

    Returns:
        list: List of related questions generated by the LLM.
    """
    prompt = """
    You are a knowledgeable financial research assistant. 
    Your users are inquiring about an annual report. 
    For the given question, propose up to five related questions to assist them in finding the information they need. 
    Provide concise, single-topic questions (without compounding sentences) that cover various aspects of the topic. 
    Ensure each question is complete and directly related to the original inquiry. 
    List each question on a separate line without numbering.
    """  # Define prompt for the LLM

    messages = [
        {
            "role": "system",
            "content": prompt,
        },
        {"role": "user", "content": query},
    ]  # Define conversation messages for the LLM

    response = client.chat.completions.create(
        model=model,
        messages=messages,
    )  # Send the query to OpenAI and get a response
    content = response.choices[0].message.content.split("\n")  # Extract and split the response
    return content

# ==========================
# 7. Visualisation of Embeddings
# ==========================
def visualise_embeddings(embeddings, retrieved_embeddings, query_embeddings, augmented_query_embeddings, title):
    """
    Visualise embeddings in a 2D space using UMAP.

    Args:
        embeddings (ndarray): The dataset embeddings.
        retrieved_embeddings (ndarray): The embeddings of retrieved documents.
        query_embeddings (ndarray): The embeddings of the original query.
        augmented_query_embeddings (ndarray): The embeddings of the augmented queries.
        title (str): The title for the plot.
    """
    plt.figure()
    plt.scatter(
        embeddings[:, 0], embeddings[:, 1], s=10, color="gray", label="Dataset Embeddings"
    )  # Plot dataset embeddings
    plt.scatter(
        retrieved_embeddings[:, 0],
        retrieved_embeddings[:, 1],
        s=100,
        facecolors="none",
        edgecolors="g",
        label="Retrieved Embeddings",
    )  # Highlight retrieved embeddings
    plt.scatter(
        query_embeddings[:, 0],
        query_embeddings[:, 1],
        s=150,
        marker="X",
        color="r",
        label="Original Query Embedding",
    )  # Mark original query embedding
    plt.scatter(
        augmented_query_embeddings[:, 0],
        augmented_query_embeddings[:, 1],
        s=150,
        marker="X",
        color="orange",
        label="Augmented Query Embeddings",
    )  # Mark augmented query embeddings
    plt.gca().set_aspect("equal", "datalim")
    plt.title(title)  # Set plot title
    plt.legend()  # Add legend
    plt.axis("off")  # Turn off axes
    plt.show()  # Display the plot

# ==========================
# 8. Main Function
# ==========================
def main():
    query = "What has been the investment in research and development?"
    results = chroma_collection.query(query_texts=[query], n_results=10, include=["documents", "embeddings"])

    retrieved_documents = results["documents"][0]
    for doc in retrieved_documents:
        print(word_wrap(doc), "\n")

    # Rerank using CrossEncoder
    cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    pairs = [[query, doc] for doc in retrieved_documents]
    scores = cross_encoder.predict(pairs)

    print("Scores:")
    for score in scores:
        print(score)

    # Multi-query Expansion
    original_query = "What were the most important factors that contributed to increases in revenue?"
    generated_queries = [
        "What were the major drivers of revenue growth?",
        "Were there any new product launches that contributed to the increase in revenue?",
        "Did any changes in pricing or promotions impact the revenue growth?",
        "What were the key market trends that facilitated the increase in revenue?",
        "Did any acquisitions or partnerships contribute to the revenue growth?",
    ]

    joint_query = [original_query] + generated_queries
    results = chroma_collection.query(query_texts=joint_query, n_results=10, include=["documents", "embeddings"])

    unique_documents = set()
    for doc_set in results["documents"]:
        unique_documents.update(doc_set)

    unique_documents = list(unique_documents)
    pairs = [[original_query, doc] for doc in unique_documents]
    rerank_scores = cross_encoder.predict(pairs)

    top_indices = np.argsort(rerank_scores)[::-1][:5]
    top_documents = [unique_documents[i] for i in top_indices]
    context = "\n\n".join(top_documents)

    # Generate Final Answer
    def generate_answer(query, context):
        prompt = f"""
        You are a knowledgeable assistant. Based on the following context, answer the user's question:

        Context:
        {context}

        Question:
        {query}
        """
        messages = [{"role": "system", "content": prompt}]
        response = client.chat.completions.create(model="gpt-3.5-turbo", messages=messages)
        return response.choices[0].message.content

    final_answer = generate_answer(original_query, context)
    print("\nFinal Answer:")
    print(final_answer)

    # Visualise embeddings
    embeddings = np.array(chroma_collection.get(include=["embeddings"])["embeddings"])
    umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)
    projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)

    original_query_embedding = embedding_function([original_query])
    augmented_query_embeddings = embedding_function(joint_query)

    project_original_query = project_embeddings(original_query_embedding, umap_transform)
    project_augmented_queries = project_embeddings(augmented_query_embeddings, umap_transform)

    retrieved_embeddings = results["embeddings"]
    result_embeddings = [item for sublist in retrieved_embeddings for item in sublist]
    projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)

    visualise_embeddings(
        projected_dataset_embeddings,
        projected_result_embeddings,
        project_original_query,
        project_augmented_queries,
        "Query and Document Embeddings",
    )

if __name__ == "__main__":
    main()


# ==========================
# 9. Entry Point
# ==========================
if __name__ == "__main__":
    main()

"""
Advanced Query Expansion with Hypothetical Answers

This script demonstrates an advanced retrieval-augmented generation (RAG) pipeline where a query is expanded using a 
hypothetical answer generated by an LLM. Both the original query and the hypothetical answer are fed into a vector store 
for enhanced document retrieval, followed by final answer generation.

The process includes:
1. Loading and preprocessing a PDF document.
2. Splitting text into manageable chunks using advanced text-splitting techniques.
3. Generating embeddings for text chunks and storing them in a ChromaDB vector store.
4. Using an LLM to augment the query with a hypothetical answer.
5. Querying the vector store with the expanded query.
6. Visualising query embeddings and retrieved document embeddings in a 2D space using UMAP.

"""

# ==========================
# 1. Import Libraries
# ==========================
import os
import umap
import matplotlib.pyplot as plt
from pypdf import PdfReader
from dotenv import load_dotenv
from openai import OpenAI
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    SentenceTransformersTokenTextSplitter,
)
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from helper_utils import project_embeddings, word_wrap
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

# ==========================
# 2. Configuration Setup
# ==========================
# Load environment variables from .env file
load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")  # Retrieve OpenAI API key from .env file
client = OpenAI(api_key=openai_key)  # Initialise OpenAI client

# ==========================
# 3. Load and Preprocess PDF
# ==========================
pdf_path = "data/microsoft-annual-report.pdf"  # Path to the PDF document
reader = PdfReader(pdf_path)  # Read the PDF file
pdf_texts = [p.extract_text().strip() for p in reader.pages if p.extract_text()]  # Extract text from each page

# ==========================
# 4. Text Splitting
# ==========================
# Character-based splitting
character_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". ", " ", ""], chunk_size=1000, chunk_overlap=0
)  # Define splitting strategy based on character chunks
character_split_texts = character_splitter.split_text("\n\n".join(pdf_texts))  # Split extracted text

# Token-based splitting
token_splitter = SentenceTransformersTokenTextSplitter(
    chunk_overlap=0, tokens_per_chunk=256
)  # Define token-based splitting strategy
token_split_texts = []
for text in character_split_texts:
    token_split_texts += token_splitter.split_text(text)  # Further split using token-based strategy

# ==========================
# 5. ChromaDB Setup and Embedding Generation
# ==========================
# Initialise embedding function and vector store
embedding_function = SentenceTransformerEmbeddingFunction()  # Set up embedding function
chroma_client = chromadb.Client()  # Initialise ChromaDB client
chroma_collection = chroma_client.create_collection(
    "microsoft-collection", embedding_function=embedding_function
)  # Create a collection in the vector store

# Generate and add embeddings to the vector store
ids = [str(i) for i in range(len(token_split_texts))]  # Create unique IDs for each chunk
chroma_collection.add(ids=ids, documents=token_split_texts)  # Add documents and embeddings to the collection

# ==========================
# 6. Query Expansion with LLM
# ==========================
def augment_query_generated(query, model="gpt-3.5-turbo"):
    """
    Augments the query by generating a hypothetical answer using an LLM.

    Args:
        query (str): The original query.
        model (str): The LLM model to use.

    Returns:
        str: The hypothetical answer generated by the LLM.
    """
    prompt = """You are a helpful expert financial research assistant. 
   Provide an example answer to the given question, that might be found in a document like an annual report."""  # Prompt for the LLM
    messages = [
        {
            "role": "system",
            "content": prompt,
        },
        {"role": "user", "content": query},
    ]  # Define conversation messages for the LLM

    response = client.chat.completions.create(
        model=model,
        messages=messages,
    )  # Send the query to OpenAI and get a response
    content = response.choices[0].message.content  # Extract the content of the response
    return content

# ==========================
# 7. Visualisation of Embeddings
# ==========================
def visualise_embeddings(embeddings, retrieved_embeddings, query_embeddings, augmented_query_embeddings, title):
    """
    Visualise embeddings in a 2D space using UMAP.

    Args:
        embeddings (ndarray): The dataset embeddings.
        retrieved_embeddings (ndarray): The embeddings of retrieved documents.
        query_embeddings (ndarray): The embeddings of the original query.
        augmented_query_embeddings (ndarray): The embeddings of the augmented query.
        title (str): The title for the plot.
    """
    plt.figure()
    plt.scatter(
        embeddings[:, 0], embeddings[:, 1], s=10, color="gray", label="Dataset Embeddings"
    )  # Plot dataset embeddings
    plt.scatter(
        retrieved_embeddings[:, 0],
        retrieved_embeddings[:, 1],
        s=100,
        facecolors="none",
        edgecolors="g",
        label="Retrieved Embeddings",
    )  # Highlight retrieved embeddings
    plt.scatter(
        query_embeddings[:, 0],
        query_embeddings[:, 1],
        s=150,
        marker="X",
        color="r",
        label="Original Query Embedding",
    )  # Mark original query embedding
    plt.scatter(
        augmented_query_embeddings[:, 0],
        augmented_query_embeddings[:, 1],
        s=150,
        marker="X",
        color="orange",
        label="Augmented Query Embedding",
    )  # Mark augmented query embedding
    plt.gca().set_aspect("equal", "datalim")
    plt.title(title)  # Set plot title
    plt.legend()  # Add legend
    plt.axis("off")  # Turn off axes
    plt.show()  # Display the plot

# ==========================
# 8. Main Function
# ==========================
def main():
    """
    Main function to execute the advanced query expansion pipeline.
    """
    # Define the original query
    original_query = (
        "What was the total profit for the year, and how does it compare to the previous year?"
    )

    # Generate a hypothetical answer to augment the query
    hypothetical_answer = augment_query_generated(original_query)  # Use LLM to generate a hypothetical answer
    joint_query = f"{original_query} {hypothetical_answer}"  # Combine original query and hypothetical answer
    print(word_wrap(joint_query))  # Print the combined query

    # Query the vector store with the expanded query
    results = chroma_collection.query(
        query_texts=[joint_query], n_results=5, include=["documents", "embeddings"]
    )  # Retrieve documents based on the expanded query
    retrieved_documents = results["documents"][0]  # Extract the retrieved documents

    # Visualise embeddings
    embeddings = chroma_collection.get(include=["embeddings"])["embeddings"]  # Get dataset embeddings
    umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)  # Fit UMAP for dimensionality reduction
    projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)  # Project dataset embeddings
    projected_retrieved_embeddings = project_embeddings(
        results["embeddings"][0], umap_transform
    )  # Project retrieved document embeddings
    projected_original_query_embedding = project_embeddings(
        embedding_function([original_query]), umap_transform
    )  # Project original query embedding
    projected_augmented_query_embedding = project_embeddings(
        embedding_function([joint_query]), umap_transform
    )  # Project augmented query embedding

    visualise_embeddings(
        projected_dataset_embeddings,
        projected_retrieved_embeddings,
        projected_original_query_embedding,
        projected_augmented_query_embedding,
        original_query,
    )  # Visualise the embeddings

# ==========================
# 9. Entry Point
# ==========================
if __name__ == "__main__":
    main()

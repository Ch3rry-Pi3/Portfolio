{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073bf8f9",
   "metadata": {},
   "source": [
    "# LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    " * Direct API calls to OpenAI\n",
    " * API calls through LangChain:\n",
    "   * Prompts\n",
    "   * Models\n",
    "   * Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ff606",
   "metadata": {},
   "source": [
    "## Get your [OpenAI API Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70aa2619",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install python-dotenv\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os                                      # For interacting with the operating system, e.g., file handling\n",
    "import openai                                  # OpenAI's Python client for accessing the OpenAI API\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())                 # Reads the local .env file to load environment variables\n",
    "\n",
    "# Set the OpenAI API key from environment variables\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']  # Retrieve the API key from the environment and set it for openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a92fb-8227-4513-8950-c965b822c425",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4336d784-65c2-4a11-8489-b445b1fad177",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9cdb",
   "metadata": {},
   "source": [
    "## Chat API : OpenAI\n",
    "\n",
    "Let's start with a direct API calls to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484bfa6a",
   "metadata": {
    "height": 268,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to get a response from the OpenAI API based on a given prompt\n",
    "def get_completion(prompt, model=llm_model):\n",
    "    # Prepare the message format for the API request, indicating the user's prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Call the OpenAI API's ChatCompletion endpoint to get the model's response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,                            # The model to use for generating a response (default is llm_model)\n",
    "        messages=messages,                      # The list of messages to send to the model\n",
    "        temperature=0,                          # Controls the randomness of the response (0 means more deterministic)\n",
    "    )\n",
    "    \n",
    "    # Return the content of the response (the actual generated text from the model)\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d076ce",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b32b57a",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c34459",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b558e2",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c883dcbd",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b33f61",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ah, I'm really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80482d1",
   "metadata": {},
   "source": [
    "## Chat API : LangChain\n",
    "\n",
    "Let's try how we can do the same using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a525b58",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c5b27",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d4a269",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc0c8b8",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d07256",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57bda7d8",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a31f246",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the ChatPromptTemplate class from the langchain.prompts module\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a ChatPromptTemplate instance by passing a template string\n",
    "# The template_string should be a predefined string containing placeholders for dynamic content\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cac2cb16",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdc5566c",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the input variables from the first message of the prompt template\n",
    "# 'messages' is a list of message objects in the prompt template, where each message can have its own structure\n",
    "# 'prompt' is the specific prompt structure within the message, which holds details like input variables\n",
    "# 'input_variables' retrieves the list of variables that are expected to be filled in dynamically when using the template\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd51a93",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a string that specifies the desired style for communication\n",
    "# The string contains the language preference (American English) and the tone of communication (calm and respectful)\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48989d11",
   "metadata": {
    "height": 217,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a string that simulates a customer's email with a complaint\n",
    "# The email is written in a pirate-like tone, expressing frustration with a product issue\n",
    "# It includes multiple lines, where the backslash (\\) is used for line continuation\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dff3954f",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Format the messages in the prompt template by injecting the style and text variables\n",
    "# The 'customer_style' specifies the tone and language style, \n",
    "# while 'customer_email' contains the actual complaint message from the customer\n",
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style,  # Pass the desired communication style\n",
    "    text=customer_email    # Pass the customer's email content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c09d8b4",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "# Check the type of the customer_messages variable (should be a list of formatted messages)\n",
    "print(type(customer_messages))\n",
    "\n",
    "# Check the type of the first item in the customer_messages list (should be a formatted message object)\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e02dafa2",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "# Print the first formatted customer message to check its contents\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd789f9f",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the LLM (Language Model) to process the customer message and generate a response\n",
    "# The LLM will take the formatted message(s) and produce a response in the same style\n",
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad294407",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make things worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\n"
     ]
    }
   ],
   "source": [
    "# Print the content of the response from the LLM\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c267e5f",
   "metadata": {
    "height": 234,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the customer service reply in a casual, slightly humorous tone\n",
    "# This string contains the response explaining why the warranty doesn't cover the cleaning expenses\n",
    "# The backslashes (\\) at the end of each line ensure the string is treated as a single line in Python\n",
    "service_reply = \"\"\" \"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya! \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ff72bd1",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the style for the response, specifying that it should be in a \"polite\" pirate tone\n",
    "# The style includes a description of the tone and language, indicating the model should reply in a pirate-like voice\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d9e8f3f",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the prompt template to format the service reply with the specified pirate style\n",
    "# The 'style' argument applies the \"pirate\" tone, and the 'text' argument provides the service reply text\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,  # Set the tone to \"pirate\" style\n",
    "    text=service_reply            # Set the customer service reply as the message content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ae5552",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, valued customer! Regrettably, the warranty be not coverin' the cost o' cleanin' yer galley due to yer own negligence. Ye see, 'twas yer own doin' when ye forgot to secure the lid afore startin' the blender. 'Tis a tough break, indeed! Fare thee well, matey!\n"
     ]
    }
   ],
   "source": [
    "# Print the content of the formatted message\n",
    "# The first message in 'service_messages' is printed to see the result of applying the pirate tone\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36536e79",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1ccdff5",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a dictionary representing product information\n",
    "# - 'gift': Boolean value indicating if the product was bought as a gift (False means not a gift)\n",
    "# - 'delivery_days': Integer indicating how many days it took for the product to be delivered (5 days)\n",
    "# - 'price_value': String description of the product's value (\"pretty affordable!\")\n",
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df0f4680",
   "metadata": {
    "height": 727,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the customer review as a multi-line string\n",
    "# The review contains details about the product's features, delivery time, and price comparison\n",
    "# The backslashes (\\) are used for line continuation to treat this as a single string\n",
    "# The review mentions the product's settings, how it arrived, and how much the reviewer likes it\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "# Define the template to extract specific information from the review\n",
    "# This template will be used to format the input for the model, asking for:\n",
    "# - 'gift': Whether the item was a gift (True/False)\n",
    "# - 'delivery_days': Number of days for delivery (if not mentioned, output -1)\n",
    "# - 'price_value': Sentences related to the product's value or price (output as a list)\n",
    "# The placeholder {text} will be replaced with the actual customer review text\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2386e9c",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class to handle the prompt template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a prompt template from the previously defined review_template string\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "\n",
    "# Print the prompt template object to check its structure\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "121bb0d4",
   "metadata": {
    "height": 268,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": \"It's slightly more expensive than the other leaf blowers out there\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Format the review text using the prompt template\n",
    "# The 'text' placeholder in the template is replaced with the actual customer_review\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "# Initialize the ChatOpenAI model with specified parameters:\n",
    "# - 'temperature=0.0' makes the model's output more deterministic (less random)\n",
    "# - 'model=llm_model' specifies which language model to use\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# Send the formatted messages to the model for processing and generate a response\n",
    "response = chat(messages)\n",
    "\n",
    "# Print the content of the response from the model (the extracted information)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10de1d28",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the type of the response.content to confirm the structure of the output\n",
    "# The expected output type is a string, as it's the model's generated response in text form\n",
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a3c0609",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# You will get an error by running this line of code \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# because'gift' is not a dictionary\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 'gift' is a string\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgift\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# You will get an error by running this line of code \n",
    "# because 'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7de2b8",
   "metadata": {},
   "source": [
    "### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2e0ec49",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary classes for structured output parsing\n",
    "# ResponseSchema is used to define the structure of the expected output\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dea24b4",
   "metadata": {
    "height": 472,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a schema for the 'gift' field, specifying that it should be a True/False answer\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "\n",
    "# Define a schema for the 'delivery_days' field, specifying that it should be the number of delivery days\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "\n",
    "# Define a schema for the 'price_value' field, specifying that it should be a list of sentences about the price\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "# Group all the response schemas into a list for easier handling\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b57e1ba8",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the output parser by using the structured response schemas defined earlier\n",
    "# The parser is created to handle the specific structure of the expected responses, which are described in 'response_schemas'\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdeaf4fc",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the format instructions from the output parser\n",
    "# These instructions explain how the model should structure its output according to the defined schemas\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1eb176c5",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Print the format instructions to verify how the output should be structured\n",
    "# These instructions will help guide the model in providing the output in the desired format\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "082947fc",
   "metadata": {
    "height": 523,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a new review template that includes the format instructions\n",
    "# The template is used to format the prompt, instructing the model on how to extract specific information\n",
    "# The {text} placeholder is replaced with the actual customer review text, \n",
    "# and {format_instructions} is replaced with the format instructions generated earlier\n",
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Create a ChatPromptTemplate from the newly defined template string (review_template_2)\n",
    "# This prepares the prompt that will be sent to the model, with placeholders to be replaced by actual values\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "# Format the message by replacing the {text} and {format_instructions} placeholders with the actual customer review \n",
    "# and the format instructions respectively\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f1537a7",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the content of the first message in the 'messages' list\n",
    "# This will display the formatted message, which includes the customer review and the format instructions\n",
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b663657",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send the formatted messages to the chat model and generate a response\n",
    "# The 'chat' function processes the input messages (customer review and format instructions) and returns a model-generated response\n",
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8c3a9fe",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": 2,\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Print the content of the response from the model\n",
    "# This will show the model's output after processing the formatted messages, which should contain the extracted information\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "904f1c25",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the response content using the output parser to convert the text-based response into a structured dictionary\n",
    "# The output_parser is responsible for extracting and organizing the relevant data from the model's response based on predefined schemas\n",
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d48b647a",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the parsed output dictionary to verify the extracted information\n",
    "# This will show the structured data (like 'gift', 'delivery_days', 'price_value') extracted from the model's response\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4346150f",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the type of the parsed output dictionary to ensure it is a dictionary\n",
    "# The expected output type is a dictionary, as it holds the extracted information from the model's response\n",
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a833fcea",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the 'delivery_days' field from the parsed output dictionary to get the number of days the product took to arrive\n",
    "# This will return the value associated with 'delivery_days' in the parsed output\n",
    "output_dict.get('delivery_days')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

"""
Query Expansion with Multiple Queries

This script demonstrates a retrieval-augmented generation (RAG) pipeline where an original query is expanded using related 
questions generated by an LLM. The expanded set of queries is used to retrieve relevant documents from a vector store, 
followed by result visualisation in a 2D embedding space using UMAP.

The process includes:
1. Loading and preprocessing a PDF document.
2. Splitting text into manageable chunks using advanced text-splitting techniques.
3. Generating embeddings for text chunks and storing them in a ChromaDB vector store.
4. Expanding the query by generating related questions with an LLM.
5. Querying the vector store with the expanded queries.
6. Visualising query embeddings and retrieved document embeddings in a 2D space using UMAP.

"""

# ==========================
# 1. Import Libraries
# ==========================
import os
import umap
import matplotlib.pyplot as plt
import numpy as np
from pypdf import PdfReader
from dotenv import load_dotenv
from openai import OpenAI
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    SentenceTransformersTokenTextSplitter,
)
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from helper_utils import project_embeddings, word_wrap
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

# ==========================
# 2. Configuration Setup
# ==========================
# Load environment variables from .env file
load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")  # Retrieve OpenAI API key from .env file
client = OpenAI(api_key=openai_key)  # Initialise OpenAI client

# ==========================
# 3. Load and Preprocess PDF
# ==========================
pdf_path = "data/microsoft-annual-report.pdf"  # Path to the PDF document
reader = PdfReader(pdf_path)  # Read the PDF file
pdf_texts = [p.extract_text().strip() for p in reader.pages if p.extract_text()]  # Extract text from each page

# ==========================
# 4. Text Splitting
# ==========================
# Character-based splitting
character_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". ", " ", ""], chunk_size=1000, chunk_overlap=0
)  # Define splitting strategy based on character chunks
character_split_texts = character_splitter.split_text("\n\n".join(pdf_texts))  # Split extracted text

# Token-based splitting
token_splitter = SentenceTransformersTokenTextSplitter(
    chunk_overlap=0, tokens_per_chunk=256
)  # Define token-based splitting strategy
token_split_texts = []
for text in character_split_texts:
    token_split_texts += token_splitter.split_text(text)  # Further split using token-based strategy

# ==========================
# 5. ChromaDB Setup and Embedding Generation
# ==========================
# Initialise embedding function and vector store
embedding_function = SentenceTransformerEmbeddingFunction()  # Set up embedding function
chroma_client = chromadb.Client()  # Initialise ChromaDB client
chroma_collection = chroma_client.create_collection(
    "microsoft-collection", embedding_function=embedding_function
)  # Create a collection in the vector store

# Generate and add embeddings to the vector store
ids = [str(i) for i in range(len(token_split_texts))]  # Create unique IDs for each chunk
chroma_collection.add(ids=ids, documents=token_split_texts)  # Add documents and embeddings to the collection

# ==========================
# 6. Query Expansion with LLM
# ==========================
def generate_multi_query(query, model="gpt-3.5-turbo"):
    """
    Expands the query by generating multiple related questions using an LLM.

    Args:
        query (str): The original query.
        model (str): The LLM model to use.

    Returns:
        list: List of related questions generated by the LLM.
    """
    prompt = """
    You are a knowledgeable financial research assistant. 
    Your users are inquiring about an annual report. 
    For the given question, propose up to five related questions to assist them in finding the information they need. 
    Provide concise, single-topic questions (without compounding sentences) that cover various aspects of the topic. 
    Ensure each question is complete and directly related to the original inquiry. 
    List each question on a separate line without numbering.
    """  # Define prompt for the LLM

    messages = [
        {
            "role": "system",
            "content": prompt,
        },
        {"role": "user", "content": query},
    ]  # Define conversation messages for the LLM

    response = client.chat.completions.create(
        model=model,
        messages=messages,
    )  # Send the query to OpenAI and get a response
    content = response.choices[0].message.content.split("\n")  # Extract and split the response
    return content

# ==========================
# 7. Visualisation of Embeddings
# ==========================
def visualise_embeddings(embeddings, retrieved_embeddings, query_embeddings, augmented_query_embeddings, title):
    """
    Visualise embeddings in a 2D space using UMAP.

    Args:
        embeddings (ndarray): The dataset embeddings.
        retrieved_embeddings (ndarray): The embeddings of retrieved documents.
        query_embeddings (ndarray): The embeddings of the original query.
        augmented_query_embeddings (ndarray): The embeddings of the augmented queries.
        title (str): The title for the plot.
    """
    plt.figure()
    plt.scatter(
        embeddings[:, 0], embeddings[:, 1], s=10, color="gray", label="Dataset Embeddings"
    )  # Plot dataset embeddings
    plt.scatter(
        retrieved_embeddings[:, 0],
        retrieved_embeddings[:, 1],
        s=100,
        facecolors="none",
        edgecolors="g",
        label="Retrieved Embeddings",
    )  # Highlight retrieved embeddings
    plt.scatter(
        query_embeddings[:, 0],
        query_embeddings[:, 1],
        s=150,
        marker="X",
        color="r",
        label="Original Query Embedding",
    )  # Mark original query embedding
    plt.scatter(
        augmented_query_embeddings[:, 0],
        augmented_query_embeddings[:, 1],
        s=150,
        marker="X",
        color="orange",
        label="Augmented Query Embeddings",
    )  # Mark augmented query embeddings
    plt.gca().set_aspect("equal", "datalim")
    plt.title(title)  # Set plot title
    plt.legend()  # Add legend
    plt.axis("off")  # Turn off axes
    plt.show()  # Display the plot

# ==========================
# 8. Main Function
# ==========================
def main():
    """
    Main function to execute the query expansion pipeline.
    """
    # Define the original query
    original_query = (
        "What details can you provide about the factors that led to revenue growth?"
    )

    # Generate related questions to expand the query
    aug_queries = generate_multi_query(original_query)  # Use LLM to generate related questions

    # Print the generated queries
    print("Generated Queries:")
    for query in aug_queries:
        print(f"- {query}")

    # Combine the original query with the augmented queries
    joint_query = [original_query] + aug_queries

    # Query the vector store with the expanded queries
    results = chroma_collection.query(
        query_texts=joint_query, n_results=5, include=["documents", "embeddings"]
    )  # Retrieve documents based on the expanded queries

    # Aggregate context from all retrieved documents
    aggregated_context = set()
    for i, query in enumerate(joint_query):
        print(f"\nQuery: {query}")
        print("Results:")
        for doc in results["documents"][i]:
            aggregated_context.add(doc)
            print(word_wrap(doc))
            print("-" * 80)

    # Combine aggregated context into a single string
    final_context = "\n\n".join(aggregated_context)

    # Use the LLM to generate a final answer
    def generate_final_answer(query, context, model="gpt-3.5-turbo"):
        prompt = f"""
        You are a highly knowledgeable assistant. Based on the following context, answer the user's question:
        
        Context:
        {context}
        
        Question:
        {query}
        
        Provide a concise, clear, and accurate response.
        """
        messages = [
            {"role": "system", "content": prompt},
        ]
        response = client.chat.completions.create(
            model=model,
            messages=messages,
        )
        return response.choices[0].message.content

    # Generate the final answer
    final_answer = generate_final_answer(original_query, final_context)
    print("\nFinal Answer:")
    print(final_answer)

    # Visualise embeddings
    embeddings = chroma_collection.get(include=["embeddings"])["embeddings"]  # Get dataset embeddings
    umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)  # Fit UMAP for dimensionality reduction
    projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)  # Project dataset embeddings

    original_query_embedding = embedding_function([original_query])  # Generate embedding for the original query
    augmented_query_embeddings = embedding_function(joint_query)  # Generate embeddings for the augmented queries

    project_original_query = project_embeddings(original_query_embedding, umap_transform)  # Project original query
    project_augmented_queries = project_embeddings(
        augmented_query_embeddings, umap_transform
    )  # Project augmented queries

    retrieved_embeddings = results["embeddings"]
    result_embeddings = [item for sublist in retrieved_embeddings for item in sublist]

    projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)  # Project retrieved embeddings

    visualise_embeddings(
        projected_dataset_embeddings,
        projected_result_embeddings,
        project_original_query,
        project_augmented_queries,
        original_query,
    )  # Visualise the embeddings


# ==========================
# 9. Entry Point
# ==========================
if __name__ == "__main__":
    main()

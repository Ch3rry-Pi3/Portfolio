{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#4682B4\">**Statewise Insurance Data Analysis with LangChain and HuggingFace**</span>\n",
    "\n",
    "## <span style=\"color:#4682B4\">**Introduction**</span>\n",
    "\n",
    "This notebook demonstrates how to leverage LangChain's retrieval-based pipelines and HuggingFace models for dynamic analysis of statewise insurance data. By integrating document loaders, vector databases, and language models, it enables efficient querying and contextual responses from a dataset of PDF documents.\n",
    "\n",
    "Key highlights of this project include:\n",
    "- **PDF Document Processing**: Load and split PDF files into manageable chunks for embedding and retrieval.\n",
    "- **HuggingFace Embeddings and Models**: Generate embeddings and responses using pre-trained models like GPT-2.\n",
    "- **Similarity-Based Search**: Perform document retrieval using FAISS vector databases to identify relevant content.\n",
    "- **Custom Prompting**: Define templates to ensure focused, context-aware responses.\n",
    "- **Question-Answering Pipeline**: Combine retrieval and LLM capabilities to answer user-defined queries.\n",
    "\n",
    "This project provides a scalable approach to querying structured and unstructured data, demonstrating the potential of LangChain and HuggingFace for document intelligence tasks.\n",
    "\n",
    "1. [<span style=\"color:#4682B4\">Import Libraries</span>](#import-libraries)\n",
    "2. [<span style=\"color:#4682B4\">Load Environment Variables</span>](#load-env-vars)\n",
    "3. [<span style=\"color:#4682B4\">Load PDF Documents</span>](#load-pdf-documents)\n",
    "4. [<span style=\"color:#4682B4\">Split Documents into Chunks</span>](#split-documents)\n",
    "5. [<span style=\"color:#4682B4\">Generate Embeddings with HuggingFace</span>](#generate-embeddings)\n",
    "6. [<span style=\"color:#4682B4\">Create VectorStore with FAISS</span>](#create-vectorstore)\n",
    "7. [<span style=\"color:#4682B4\">Query Using Similarity Search</span>](#query-similarity-search)\n",
    "8. [<span style=\"color:#4682B4\">Use HuggingFaceHub for Querying</span>](#huggingfacehub-querying)\n",
    "9. [<span style=\"color:#4682B4\">Use HuggingFacePipeline for Querying</span>](#huggingfacepipeline-querying)\n",
    "10. [<span style=\"color:#4682B4\">Create a Prompt Template</span>](#create-prompt-template)\n",
    "11. [<span style=\"color:#4682B4\">Create a Retrieval-Based Question-Answering Chain</span>](#create-retrieval-qa)\n",
    "12. [<span style=\"color:#4682B4\">Execute the Question-Answering Query</span>](#execute-query)\n",
    "\n",
    "## <span style=\"color:#4682B4\">**1. Import Libraries**</span> <a id=\"import-libraries\"></a>\n",
    "\n",
    "This section includes the necessary imports for building the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for environment variable management and additional utilities\n",
    "import os                                                               # For file and path operations\n",
    "from dotenv import load_dotenv                                          # Securely load environment variables\n",
    "import requests                                                         # For making HTTP requests\n",
    "import numpy as np                                                      # For numerical operations\n",
    "\n",
    "# Import LangChain document loaders for processing PDF files\n",
    "from langchain_community.document_loaders import PyPDFLoader            # For loading individual PDF files\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader   # For loading all PDFs in a directory\n",
    "\n",
    "# Import LangChain utility for splitting documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter      # Splits documents into manageable chunks\n",
    "\n",
    "# Import LangChain vector database for storing embeddings\n",
    "from langchain_community.vectorstores import FAISS                      # Facebook's FAISS for similarity search\n",
    "\n",
    "# Import HuggingFace utilities for embeddings and endpoints\n",
    "from langchain_huggingface import HuggingFaceEmbeddings                 # Generate embeddings using HuggingFace models\n",
    "from langchain_huggingface import HuggingFaceEndpoint                   # For accessing HuggingFace endpoints\n",
    "\n",
    "# Import LangChain tools for prompting and chains\n",
    "from langchain.prompts import PromptTemplate                            # To define custom prompts\n",
    "from langchain.chains import RetrievalQA                                # Combine retriever and LLM for Q&A\n",
    "\n",
    "# Import LangChain integrations for HuggingFace models\n",
    "from langchain_community.llms import HuggingFaceHub                     # Interact with models hosted on HuggingFace Hub\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline  # Use HuggingFace pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**2. Load Environment Variables**</span> <a id=\"load-env-vars\"></a>\n",
    "\n",
    "In this section, we securely load the OpenAI API key from a `.env` file. This ensures that sensitive information, such as API keys, is not hardcoded into the script. The `dotenv` library is used to manage environment variables effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the `.env` file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Hugging Face token\n",
    "hf_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**3. Load PDF Documents**</span> <a id=\"load-pdf-documents\"></a>\n",
    "\n",
    "In this section, we use the `PyPDFDirectoryLoader` to load all PDF files from a specified directory. The loader processes the documents and stores them in a format suitable for further processing, such as text splitting and embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 63\n"
     ]
    }
   ],
   "source": [
    "# Load all PDF documents from the specified directory\n",
    "pdf_loader = PyPDFDirectoryLoader(\"./us_census\")\n",
    "\n",
    "# Load the content of the PDFs into a list of documents\n",
    "pdf_documents = pdf_loader.load()\n",
    "\n",
    "# Display the number of loaded documents to verify\n",
    "print(f\"Number of documents loaded: {len(pdf_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**4. Split Documents into Chunks**</span> <a id=\"split-documents\"></a>\n",
    "\n",
    "To process the PDF documents effectively, we split them into smaller, manageable chunks using the `RecursiveCharacterTextSplitter`. This approach ensures that each chunk is within the model's token limit while maintaining overlap to preserve context between chunks.\n",
    "\n",
    "### Parameters:\n",
    "- **`chunk_size`**: The maximum size of each chunk (in characters).\n",
    "- **`chunk_overlap`**: The overlap between consecutive chunks to retain context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Chunk:\n",
      " page_content='Health Insurance Coverage Status and Type \n",
      "by Geography: 2021 and 2022\n",
      "American Community Survey Briefs\n",
      "ACSBR-015\n",
      "Issued September 2023\n",
      "Douglas Conway and Breauna Branch\n",
      "INTRODUCTION\n",
      "Demographic shifts as well as economic and govern-\n",
      "ment policy changes can affect people’s access to \n",
      "health coverage. For example, between 2021 and 2022, \n",
      "the labor market continued to improve, which may \n",
      "have affected private coverage in the United States \n",
      "during that time.1 Public policy changes included \n",
      "the renewal of the Public Health Emergency, which \n",
      "allowed Medicaid enrollees to remain covered under \n",
      "the Continuous Enrollment Provision.2 The American \n",
      "Rescue Plan (ARP) enhanced Marketplace premium \n",
      "subsidies for those with incomes above 400 percent \n",
      "of the poverty level as well as for unemployed people.3\n",
      "In addition to national policies, individual states and \n",
      "the District of Columbia can affect health insurance \n",
      "coverage by making Marketplace or Medicaid more' metadata={'source': 'us_census\\\\acsbr-015.pdf', 'page': 0}\n",
      "Total number of chunks: 316\n"
     ]
    }
   ],
   "source": [
    "# Initialise the RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,                    # Maximum size of each document chunk (in characters)\n",
    "    chunk_overlap=200                   # Overlap between consecutive chunks to preserve context\n",
    ")\n",
    "\n",
    "# Split the loaded PDF documents into smaller chunks\n",
    "final_documents = text_splitter.split_documents(pdf_documents)\n",
    "\n",
    "# Display the first chunk to verify the output\n",
    "print(\"Sample Chunk:\\n\", final_documents[0])\n",
    "\n",
    "# Display the total number of document chunks created\n",
    "print(f\"Total number of chunks: {len(final_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**5. Generate Embeddings with HuggingFace**</span> <a id=\"generate-embeddings\"></a>\n",
    "\n",
    "In this section, we use the `HuggingFaceEmbeddings` module to create vector embeddings for the document chunks. These embeddings represent the semantic meaning of the text and can be used for similarity-based retrieval tasks.\n",
    "\n",
    "### Parameters:\n",
    "- **`model_name`**: The HuggingFace model used for embedding text.\n",
    "- **`model_kwargs`**: Additional arguments for the embedding model (e.g., device configuration).\n",
    "- **`encode_kwargs`**: Arguments for embedding behavior, such as normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\The_R\\OneDrive\\Documents\\Portfolio\\AI\\LangChain\\8. RAG using Hugging Face & Mistral\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Vector:\n",
      " [-8.30601528e-02 -1.45066781e-02 -2.10276209e-02  2.72682514e-02\n",
      "  4.53647189e-02  5.28341569e-02 -2.53759008e-02  3.61303873e-02\n",
      " -9.08312052e-02 -2.77017541e-02  7.97398016e-02  6.42475039e-02\n",
      " -3.54004540e-02 -4.04245965e-02 -1.13771809e-02  4.45296392e-02\n",
      " -3.88549455e-03 -3.79062863e-03 -4.54510041e-02  2.67047286e-02\n",
      " -2.05681734e-02  2.87432708e-02 -2.41201706e-02 -3.69412228e-02\n",
      "  1.92781072e-02  1.06194830e-02  3.21828108e-03  2.33252533e-03\n",
      " -4.29321937e-02 -1.64999187e-01  2.77012540e-03  2.68276725e-02\n",
      " -4.12894450e-02 -1.88446417e-02  1.58918686e-02  9.22324881e-03\n",
      " -2.00687312e-02  8.16561207e-02  3.89413238e-02  5.52223697e-02\n",
      " -3.69984321e-02  1.75319184e-02 -1.28967147e-02  2.80576147e-04\n",
      " -2.51581222e-02  4.59336163e-03 -2.39579082e-02 -5.76565601e-03\n",
      "  6.02953788e-03 -3.61177884e-02  3.84415388e-02 -1.75466656e-03\n",
      "  5.05656078e-02  6.02408350e-02  4.52067479e-02 -4.91435602e-02\n",
      "  1.82053614e-02 -1.46668823e-02 -2.53130607e-02  3.18243839e-02\n",
      "  5.15598357e-02 -9.32342000e-03 -2.61889279e-01  1.00012712e-01\n",
      "  1.34968543e-02  4.36564796e-02 -6.08388660e-03 -4.54127043e-03\n",
      " -3.26450169e-02 -4.28530797e-02 -5.28034791e-02  4.51760404e-02\n",
      " -4.90110703e-02  1.14362622e-02  3.36463116e-02  2.09724065e-02\n",
      "  1.38648022e-02  5.93808293e-03  1.49723291e-02 -8.11351836e-03\n",
      "  1.01395175e-02  2.79459674e-02 -6.45080674e-03 -4.83986810e-02\n",
      "  4.86142412e-02 -8.64626616e-02  5.24354205e-02 -3.35655548e-02\n",
      "  3.21200974e-02 -3.52678448e-02 -3.55471559e-02  1.33936917e-02\n",
      " -2.35929410e-03  3.77090387e-02  6.75864052e-03  3.44191752e-02\n",
      " -5.24617266e-03 -8.69448297e-03 -1.43432897e-02  3.42605770e-01\n",
      " -2.61229761e-02  9.80759691e-03 -2.25788709e-02  8.17074161e-03\n",
      " -9.10547748e-03 -6.63615689e-02 -3.31286644e-03  3.76494695e-03\n",
      "  1.78496093e-02  1.94206424e-02  1.89167485e-02 -2.34020241e-02\n",
      "  1.70285888e-02  3.59937586e-02 -5.00385836e-02 -2.86068264e-02\n",
      "  3.76006030e-02  1.95080303e-02  9.70121697e-02 -2.35506557e-02\n",
      " -5.60346060e-03  4.08104621e-02 -1.22215124e-02 -4.84442525e-02\n",
      "  1.59178283e-02  7.82314688e-02  5.22117242e-02  1.41182363e-01\n",
      "  3.60795520e-02 -3.56155448e-02  1.04116306e-01 -4.85004447e-02\n",
      " -9.04250238e-03 -3.47430352e-04 -3.86386435e-03  1.05187045e-02\n",
      " -1.18663525e-02  4.76750769e-02 -1.44307967e-02  4.32956591e-02\n",
      "  1.39371818e-02 -9.43202432e-03 -3.39477719e-03 -1.30119130e-01\n",
      " -2.24878266e-02  1.85143143e-01 -3.77383195e-02  4.94090095e-02\n",
      "  1.73504595e-02 -2.00506113e-02 -4.56999242e-02  6.15431741e-02\n",
      " -3.38922553e-02  3.41245197e-02 -3.52289751e-02  2.25264821e-02\n",
      " -3.88040370e-03  2.96609048e-02 -3.23118232e-02 -5.61316796e-02\n",
      "  5.52870706e-02 -3.27033848e-02 -4.70694825e-02  1.33565273e-02\n",
      "  4.41281721e-02 -2.02731118e-02 -1.62062887e-02 -5.46823218e-02\n",
      "  2.79455427e-02  9.90040321e-03  1.44527284e-02  5.46371862e-02\n",
      "  2.29170453e-02 -2.23384183e-02  9.18255672e-02  2.43632048e-02\n",
      " -3.52479368e-02 -8.27396568e-03 -5.45697939e-03 -5.42017259e-02\n",
      " -5.69414580e-03 -2.11965349e-02 -5.15156835e-02 -4.83904891e-02\n",
      " -1.58691462e-02 -3.49855684e-02 -6.40029311e-02  4.69647311e-02\n",
      "  5.08958101e-02 -1.47770653e-02 -2.62470171e-03  9.61362105e-03\n",
      " -5.30825891e-02  3.27618532e-02 -1.20410938e-02  1.89259148e-03\n",
      " -6.53932840e-02 -2.97078118e-02  4.61532064e-02 -3.11540384e-02\n",
      " -3.44942473e-02  2.73555480e-02  1.33986240e-02 -1.12299845e-02\n",
      " -5.48389042e-03  1.20181991e-02  3.20996828e-02 -7.33810291e-02\n",
      "  4.33215164e-02 -1.46467052e-02 -6.26266189e-03  4.07445841e-02\n",
      "  2.55041532e-02  1.34794565e-03  3.32123823e-02  2.35810634e-02\n",
      "  1.29890274e-02 -2.96592526e-02  9.96660208e-04  3.23757753e-02\n",
      "  3.47310156e-02  7.43660331e-02  8.83725658e-02 -2.74852782e-01\n",
      " -5.02357213e-03  6.73113298e-03  5.10838954e-03 -7.25276768e-02\n",
      " -5.16856126e-02 -3.50604393e-02  2.54638959e-02  5.96715743e-03\n",
      "  9.32259634e-02  6.52372465e-02 -1.24274120e-02 -2.26455219e-02\n",
      "  1.03852808e-01  2.69623939e-02 -5.88307604e-02  3.26073729e-02\n",
      " -2.70966943e-02 -1.62646528e-02  3.82237509e-03  1.43360381e-03\n",
      " -1.74212304e-03 -5.87846600e-02 -2.49719346e-06  8.17155242e-02\n",
      " -8.29971454e-04  6.18573502e-02 -5.67798018e-02 -7.95386210e-02\n",
      "  8.72029364e-03 -5.23827635e-02  4.91903797e-02 -1.34030152e-02\n",
      " -1.09614663e-01  6.37905523e-02  1.59761664e-02 -8.35732445e-02\n",
      " -1.80335678e-02 -5.23566790e-02 -2.67080627e-02 -2.57244967e-02\n",
      "  5.58587015e-02 -6.18519373e-02  1.82368793e-02 -1.30652115e-02\n",
      " -5.21241277e-02  4.50306796e-02  6.84901997e-02 -7.28626102e-02\n",
      "  4.47438518e-03  8.44439492e-03 -2.58527976e-02 -2.72554457e-02\n",
      " -8.25328194e-03  2.65439153e-02 -6.06657900e-02 -3.46958898e-02\n",
      "  2.65969113e-02 -1.06718708e-02 -6.09567203e-03  2.14285702e-02\n",
      "  4.35664179e-03  3.98480631e-02 -7.30910059e-03  7.62269541e-04\n",
      " -4.81284596e-02  9.06977337e-03 -2.74650380e-02 -6.46280050e-02\n",
      "  7.81241385e-03  6.36935059e-04  3.72856259e-02 -2.43166685e-02\n",
      " -2.94920485e-02  2.59610172e-02 -2.04743184e-02  2.99483538e-02\n",
      " -3.93522531e-03 -7.24740140e-03 -4.70116399e-02  4.60672714e-02\n",
      " -6.55132532e-02  1.94678400e-02  4.70717810e-02 -4.82361531e-03\n",
      "  1.85413435e-02  1.27155315e-02  9.35990922e-03 -1.63067877e-02\n",
      "  1.78641472e-02 -1.42972181e-02 -1.85641684e-02 -1.11412806e-02\n",
      " -2.76978519e-02 -1.63668836e-03  1.32831978e-02 -2.23142445e-01\n",
      "  5.02429828e-02  3.20567936e-02 -2.58731581e-02  7.37163099e-03\n",
      " -1.69072766e-02 -1.84587017e-02  1.52460774e-02 -1.25951674e-02\n",
      " -1.74231697e-02  4.49848324e-02  8.47080350e-02  1.25501946e-01\n",
      " -1.12586841e-02 -9.03828163e-03 -4.29149438e-03  7.40758330e-02\n",
      " -5.70529047e-03  2.40515228e-02 -4.09415625e-02  5.69055639e-02\n",
      " -6.37594387e-02  1.52565360e-01 -1.74385291e-02  1.57483034e-02\n",
      " -7.21550584e-02 -2.26559564e-02  3.81210186e-02 -4.17390764e-02\n",
      "  1.38446465e-02  1.97878517e-02  1.08574498e-02  1.14633823e-02\n",
      " -1.90664809e-02  6.13893457e-02  4.67905682e-03  1.12023829e-02\n",
      "  4.75127958e-02  4.35522525e-03  3.34495492e-02 -5.18546775e-02\n",
      " -1.82738230e-02  9.24014114e-03  2.57453695e-03  6.83404282e-02\n",
      " -3.80241126e-02 -5.42043075e-02 -7.67753124e-02 -8.67900718e-03\n",
      "  6.64248466e-02  9.31191165e-03 -3.73430178e-02 -2.34107277e-03\n",
      " -5.78066520e-03 -5.70392124e-02 -6.05430454e-03 -5.90126775e-02\n",
      "  3.92624699e-02 -8.48793052e-03 -1.47276372e-02  2.09183879e-02\n",
      "  5.32475747e-02 -7.08249062e-02  1.93986539e-02  6.82188123e-02]\n",
      "Embedding Vector Shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFaceEmbeddings for vectorization\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",        # Pretrained HuggingFace model for embeddings\n",
    "    model_kwargs={\n",
    "        'device': 'cpu'                         # Specify the device for embedding (e.g., CPU or GPU)\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        'normalize_embeddings': True            # Normalize the embeddings for better similarity comparison\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generate embeddings for the first document chunk\n",
    "chunk_embedding = np.array(huggingface_embeddings.embed_query(final_documents[0].page_content))\n",
    "print(\"Embedding Vector:\\n\", chunk_embedding)\n",
    "\n",
    "# Display the shape of the embedding vector\n",
    "print(f\"Embedding Vector Shape: {chunk_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**6. Create VectorStore with FAISS**</span> <a id=\"create-vectorstore\"></a>\n",
    "\n",
    "In this section, we create a FAISS vector store to store the embeddings of the document chunks. FAISS is a library for efficient similarity search and clustering of dense vectors, enabling fast retrieval of relevant document chunks.\n",
    "\n",
    "- **`final_documents[:120]`**: The first 120 document chunks used to create the vector store.\n",
    "- **`huggingface_embeddings`**: The embedding model used to generate the vector representations of the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS VectorStore successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Create a FAISS vector store for document embeddings\n",
    "vectorstore = FAISS.from_documents(\n",
    "    final_documents[:120],               # Use the first 120 document chunks\n",
    "    huggingface_embeddings               # Pretrained HuggingFace embedding model\n",
    ")\n",
    "\n",
    "# Print confirmation of VectorStore creation\n",
    "print(\"FAISS VectorStore successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**7. Query Using Similarity Search**</span> <a id=\"query-similarity-search\"></a>\n",
    "\n",
    "In this section, we perform a similarity search on the vector store using a natural language query. The query retrieves the most relevant document chunks based on their embeddings.\n",
    "\n",
    "### Steps:\n",
    "1. **Perform Similarity Search**: Retrieve relevant documents based on the query.\n",
    "2. **Initialise a Retriever**: Convert the vector store into a retriever for more advanced retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Document:\n",
      " 2 U.S. Census Bureau\n",
      "WHAT IS HEALTH INSURANCE COVERAGE?\n",
      "This brief presents state-level estimates of health insurance coverage \n",
      "using data from the American Community Survey (ACS). The  \n",
      "U.S. Census Bureau conducts the ACS throughout the year; the \n",
      "survey asks respondents to report their coverage at the time of \n",
      "interview. The resulting measure of health insurance coverage, \n",
      "therefore, reflects an annual average of current comprehensive \n",
      "health insurance coverage status.* This uninsured rate measures a \n",
      "different concept than the measure based on the Current Population \n",
      "Survey Annual Social and Economic Supplement (CPS ASEC). \n",
      "For reporting purposes, the ACS broadly classifies health insurance \n",
      "coverage as private insurance or public insurance. The ACS defines \n",
      "private health insurance as a plan provided through an employer \n",
      "or a union, coverage purchased directly by an individual from an \n",
      "insurance company or through an exchange (such as healthcare.\n",
      "Retriever Object:\n",
      " tags=['FAISS', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000014469ECB4F0> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "# Define a natural language query\n",
    "query = \"What is health insurance coverage?\"  # User's query for retrieving relevant documents\n",
    "\n",
    "# Perform a similarity search on the vector store\n",
    "relevant_documents = vectorstore.similarity_search(query)\n",
    "\n",
    "# Display the content of the most relevant document chunk\n",
    "print(\"Most Relevant Document:\\n\", relevant_documents[0].page_content)\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",             # Specify similarity-based retrieval\n",
    "    search_kwargs={\"k\": 3}                # Retrieve the top 3 most similar documents\n",
    ")\n",
    "\n",
    "# Display the retriever object to verify setup\n",
    "print(\"Retriever Object:\\n\", retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**8. Use HuggingFaceHub for Querying**</span> <a id=\"huggingfacehub-querying\"></a>\n",
    "\n",
    "In this section, we use the `HuggingFaceHub` module to interact with a pre-trained model hosted on Hugging Face. This allows us to generate responses to user queries based on the model's capabilities.\n",
    "\n",
    "- **`repo_id`**: The Hugging Face repository ID for the model to use (e.g., `gpt2`).\n",
    "- **`model_kwargs`**: Additional arguments for model behavior, such as temperature and maximum response length.\n",
    "- **`huggingfacehub_api_token`**: The API token for authenticating with Hugging Face's Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\The_R\\AppData\\Local\\Temp\\ipykernel_22848\\405748289.py:2: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  hf = HuggingFaceHub(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:\n",
      " What is the health insurance coverage?\n",
      "\n",
      "The Affordable Care Act (ACA) provides coverage for people with pre-existing conditions. The ACA also provides coverage for people with pre-existing conditions that are covered under Medicaid.\n",
      "\n",
      "What is the cost of coverage?\n",
      "\n",
      "The cost of coverage is determined by the individual's income, the cost of health insurance, and the cost of the coverage.\n",
      "\n",
      "What is the cost of coverage for a family of four?\n",
      "\n",
      "The cost of coverage for a family of four is determined by the individual's income, the cost of health insurance, and the cost of the coverage.\n",
      "\n",
      "What is the cost of coverage for a family of four with a dependent child?\n",
      "\n",
      "The cost of coverage for a family of four with a dependent child is determined by the individual's income, the cost of health insurance, and the cost of the coverage.\n",
      "\n",
      "What is the cost of coverage for a family of four with a dependent child with a pre-existing condition?\n",
      "\n",
      "The cost of coverage for a family of four with a pre-existing condition is determined by the individual's income, the cost of health insurance, and the cost of the coverage.\n",
      "\n",
      "What is the cost of coverage for a family of four with a pre-existing condition with a pre-existing condition?\n",
      "\n",
      "The cost of coverage for a family of four with a pre-existing condition with a pre-existing condition is determined by the individual's income, the cost of health insurance, and the cost of the coverage.\n",
      "\n",
      "What is the cost of coverage for a family of four with a pre-existing condition with a pre-existing condition?\n",
      "\n",
      "The cost of coverage for a family of four with a pre-existing condition with a pre-existing condition is determined by the individual's income, the cost of health insurance, and the cost of the coverage.\n",
      "\n",
      "What is the cost of coverage for a family of four with a pre-existing condition with a pre-existing condition?\n",
      "\n",
      "The cost of coverage for a family of four with a pre-existing condition with a pre-existing condition is determined by the individual's income, the cost of health insurance, and the cost of the coverage.\n",
      "\n",
      "What is the cost of coverage for a family of four with a pre-existing condition with a pre-existing condition with a pre-existing condition?\n",
      "\n",
      "The cost of coverage for a family of four with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with a pre-existing condition with\n"
     ]
    }
   ],
   "source": [
    "# Initialise HuggingFaceHub with the specified model and API token\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id=\"gpt2\",                                 # Hugging Face model repository ID (e.g., GPT-2)\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,                         # Sampling temperature for more deterministic responses\n",
    "        \"max_length\": 500                           # Maximum length of the generated response\n",
    "    },\n",
    "    huggingfacehub_api_token=hf_api_token           # API token for authenticating with Hugging Face Hub\n",
    ")\n",
    "\n",
    "# Define the query for the model\n",
    "query = \"What is the health insurance coverage?\"    # User's query to be processed by the Hugging Face model\n",
    "\n",
    "# Invoke the Hugging Face model with the query\n",
    "response = hf.invoke(query)\n",
    "\n",
    "# Display the model's response\n",
    "print(\"Model Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**9. Use HuggingFacePipeline for Querying**</span> <a id=\"huggingfacepipeline-querying\"></a>\n",
    "\n",
    "In this section, we use the `HuggingFacePipeline` module to interact with a pre-trained model from Hugging Face. This method utilizes pipelines for tasks such as text generation, enabling dynamic responses to queries.\n",
    "\n",
    "- **`model_id`**: The identifier of the model to use (e.g., `gpt2`).\n",
    "- **`task`**: Specifies the type of pipeline task (e.g., `text-generation`).\n",
    "- **`pipeline_kwargs`**: Additional arguments for pipeline configuration, such as temperature and maximum tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:\n",
      " What is the health insurance coverage?\n",
      "\n",
      "The Affordable Care Act (ACA) requires that all Americans have health insurance. The ACA also requires that all Americans have health insurance.\n",
      "\n",
      "What is the cost of health insurance?\n",
      "\n",
      "The cost of health insurance is the cost of the cost of health insurance.\n",
      "\n",
      "What is the cost of health insurance?\n",
      "\n",
      "The cost of health insurance is the cost of the cost of health insurance.\n",
      "\n",
      "What is the cost of health insurance?\n",
      "\n",
      "The cost of health insurance is\n"
     ]
    }
   ],
   "source": [
    "# Initialise the HuggingFacePipeline for text generation\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",                           # Hugging Face model ID (e.g., GPT-2)\n",
    "    task=\"text-generation\",                    # Specify the task (e.g., text generation)\n",
    "    pipeline_kwargs={\n",
    "        \"temperature\": 0.1,                    # Sampling temperature for more deterministic responses\n",
    "        \"max_new_tokens\": 100                  # Maximum number of new tokens to generate\n",
    "    }\n",
    ")\n",
    "\n",
    "# Invoke the pipeline model with the query\n",
    "response = hf.invoke(query)                    # Process the query using the initialised model\n",
    "\n",
    "# Display the generated response\n",
    "print(\"Model Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**10. Create a Prompt Template**</span> <a id=\"create-prompt-template\"></a>\n",
    "\n",
    "In this section, we define a `PromptTemplate` to structure the input for the language model. The template includes placeholders for context and the user's question, ensuring that the model generates responses based solely on the provided context.\n",
    "\n",
    "- **`template`**: The format of the prompt, including placeholders for input variables.\n",
    "- **`input_variables`**: A list of placeholders (e.g., `context`, `question`) that the template will populate dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Prompt Template:\n",
      " input_variables=['context', 'question'] input_types={} partial_variables={} template='\\nUse the following piece of context to answer the question asked.\\nPlease try to provide the answer only based on the context.\\n\\n{context}\\nQuestion: {question}\\n\\nHelpful Answers:\\n'\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template for interacting with the language model\n",
    "prompt_template = \"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context.\n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answers:\n",
    "\"\"\"\n",
    "\n",
    "# Initialise the PromptTemplate with the defined template\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,                       # The template string with placeholders\n",
    "    input_variables=[\"context\", \"question\"]         # List of placeholders in the template\n",
    ")\n",
    "\n",
    "# Display the initialised prompt to verify its structure\n",
    "print(\"Initialised Prompt Template:\\n\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**11. Create a Retrieval-Based Question-Answering Chain**</span> <a id=\"create-retrieval-qa\"></a>\n",
    "\n",
    "In this section, we create a `RetrievalQA` chain to combine a retriever and a language model for question answering. The chain retrieves relevant documents based on the query and uses the provided prompt to generate responses.\n",
    "\n",
    "- **`llm`**: The language model used to generate answers.\n",
    "- **`chain_type`**: Specifies the type of chain to use (e.g., `\"stuff\"` combines all documents into one).\n",
    "- **`retriever`**: The retriever object for fetching relevant documents.\n",
    "- **`return_source_documents`**: If `True`, returns the documents used to generate the response.\n",
    "- **`chain_type_kwargs`**: Additional parameters, such as the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RetrievalQA Chain:\n",
      " verbose=False combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nUse the following piece of context to answer the question asked.\\nPlease try to provide the answer only based on the context.\\n\\n{context}\\nQuestion: {question}\\n\\nHelpful Answers:\\n'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x0000014469954340>, model_kwargs={}, pipeline_kwargs={'temperature': 0.1, 'max_new_tokens': 100}), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') return_source_documents=True retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000014469ECB4F0>, search_kwargs={'k': 3})\n"
     ]
    }
   ],
   "source": [
    "# Create a RetrievalQA chain for question answering\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=hf,                                 # The language model for generating answers\n",
    "    chain_type=\"stuff\",                     # Combine all retrieved documents into a single context\n",
    "    retriever=retriever,                    # Retriever object for fetching relevant documents\n",
    "    return_source_documents=True,           # Include the source documents in the response\n",
    "    chain_type_kwargs={\"prompt\": prompt}    # Use the defined prompt template for formatting queries\n",
    ")\n",
    "\n",
    "# Display the RetrievalQA chain to verify setup\n",
    "print(\"Initialised RetrievalQA Chain:\\n\", retrievalQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**12. Execute the Question-Answering Query**</span> <a id=\"execute-query\"></a>\n",
    "\n",
    "In this section, we use the `RetrievalQA` chain to execute a query and retrieve context-aware responses. The query is processed using the retriever to fetch relevant documents, and the language model generates the final answer based on the prompt template.\n",
    "\n",
    "### Steps:\n",
    "1. Define the query.\n",
    "2. Invoke the `RetrievalQA` chain with the query.\n",
    "3. Display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA Chain Result:\n",
      " \n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context.\n",
      "\n",
      "percent (Appendix Table B-5). \n",
      "Medicaid coverage accounted \n",
      "for a portion of that difference. \n",
      "Medicaid coverage was 22.7 per-\n",
      "cent in the group of states that \n",
      "expanded Medicaid eligibility and \n",
      "18.0 percent in the group of nonex-\n",
      "pansion states.\n",
      "CHANGES IN THE UNINSURED \n",
      "RATE BY STATE FROM 2021 \n",
      "TO 2022\n",
      "From 2021 to 2022, uninsured rates \n",
      "decreased across 27 states, while \n",
      "only Maine had an increase. The \n",
      "uninsured rate in Maine increased \n",
      "from 5.7 percent to 6.6 percent, \n",
      "although it remained below the \n",
      "national average. Maine’s uninsured \n",
      "rate was still below 8.0 percent, \n",
      "21 Douglas Conway and Breauna Branch, \n",
      "“Health Insurance Coverage Status and Type \n",
      "by Geography: 2019 and 2021,” 2022, <www.\n",
      "census.gov/content/dam/Census/library/\n",
      "publications/2022/acs/acsbr-013.pdf>.\n",
      "\n",
      "10 U.S. Census Bureau\n",
      "SUMMARY\n",
      "The uninsured rate fell in 27 states \n",
      "(mainly states that had expanded \n",
      "Medicaid eligibility), while only \n",
      "Maine had an increase of 0.8 \n",
      "percentage points. Only one state \n",
      "saw a decrease in public coverage \n",
      "(Rhode Island), while seven states \n",
      "experienced decreases in private \n",
      "coverage. As groups, states that \n",
      "expanded Medicaid eligibility saw \n",
      "an increase in public coverage, \n",
      "while states that did not expand \n",
      "Medicaid eligibility saw an increase \n",
      "in private coverage from 2021 to \n",
      "2022, although expansion states \n",
      "had both higher private and public \n",
      "coverage rates than nonexpansion \n",
      "states to start with in both 2021 and \n",
      "2022. Massachusetts had the low-\n",
      "est uninsured rate and Texas had \n",
      "the highest in 2022.37 In 2022, Utah \n",
      "had the highest private coverage \n",
      "and lowest public coverage rate, \n",
      "while New Mexico had the high-\n",
      "est public coverage and the lowest \n",
      "private coverage rate, consistent \n",
      "37 The uninsured rates in the District\n",
      "\n",
      "8 U.S. Census Bureau\n",
      "which was the state’s uninsured \n",
      "rate in 2019, the year before it \n",
      "expanded Medicaid eligibility.22\n",
      "Declines in the uninsured rate in \n",
      "the majority of these states were \n",
      "related to changes in their public \n",
      "and private coverage rates. For \n",
      "seven of the states with lower \n",
      "uninsured rates in 2022, the dif-\n",
      "ference was driven by increases \n",
      "in private coverage. These states \n",
      "were Florida, Kansas, Mississippi, \n",
      "North Carolina, Ohio, South \n",
      "Carolina, and Texas.\n",
      "For seven states, the uninsured \n",
      "rate decrease was related to \n",
      "increases in public coverage with \n",
      "no corresponding change in the \n",
      "level of private coverage. These \n",
      "states were Alabama, California, \n",
      "Georgia, Illinois, Indiana, Michigan, \n",
      "and Oklahoma. In three states \n",
      "(Missouri, New York, and Virginia), \n",
      "it was shifts in coverage from pri-\n",
      "vate to public that contributed to \n",
      "the decline in their uninsured rates. \n",
      "The uninsured rate in expansion \n",
      "states as a group decreased from \n",
      "6.8 percent to 6.3 percent; non-\n",
      "Question: \n",
      "What are the differences in\n",
      "uninsured rate by state in 2022\n",
      "\n",
      "\n",
      "Helpful Answers:\n",
      "\n",
      "1. The uninsured rate in the\n",
      "\n",
      "majority of states was \n",
      "6.8 percent in the group of states that \n",
      "\n",
      "expanded Medicaid eligibility.\n",
      "\n",
      "2. The uninsured rate in the\n",
      "\n",
      "majority of states was \n",
      "6.8 percent in the group of states that \n",
      "\n",
      "expanded Medicaid eligibility.\n",
      "\n",
      "3. The uninsured rate in the\n",
      "\n",
      "majority of states was \n",
      "\n",
      "6.8 percent in the group of states that \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the query to retrieve information\n",
    "query = \"\"\"\n",
    "What are the differences in\n",
    "uninsured rate by state in 2022\n",
    "\"\"\"\n",
    "\n",
    "# Call the RetrievalQA chain with the query\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "\n",
    "# Display the result generated by the QA chain\n",
    "print(\"QA Chain Result:\\n\", result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

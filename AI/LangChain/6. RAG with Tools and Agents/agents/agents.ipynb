{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#4682B4\">**Statewise Insurance Data Analysis with LangChain and HuggingFace**</span>\n",
    "\n",
    "## <span style=\"color:#4682B4\">**Introduction**</span>\n",
    "\n",
    "This notebook demonstrates how to leverage LangChain's retrieval-based pipelines and HuggingFace models for dynamic analysis of statewise insurance data. By integrating document loaders, vector databases, and language models, it enables efficient querying and contextual responses from a dataset of PDF documents.\n",
    "\n",
    "Key highlights of this project include:\n",
    "- **PDF Document Processing**: Load and split PDF files into manageable chunks for embedding and retrieval.\n",
    "- **HuggingFace Embeddings and Models**: Generate embeddings and responses using pre-trained models like GPT-2.\n",
    "- **Similarity-Based Search**: Perform document retrieval using FAISS vector databases to identify relevant content.\n",
    "- **Custom Prompting**: Define templates to ensure focused, context-aware responses.\n",
    "- **Question-Answering Pipeline**: Combine retrieval and LLM capabilities to answer user-defined queries.\n",
    "\n",
    "This project provides a scalable approach to querying structured and unstructured data, demonstrating the potential of LangChain and HuggingFace for document intelligence tasks.\n",
    "\n",
    "## <span style=\"color:#4682B4\">**Table of Contents**</span>\n",
    "\n",
    "1. [<span style=\"color:#4682B4\">Import Libraries</span>](#import-libraries)\n",
    "2. [<span style=\"color:#4682B4\">Load Environment Variables</span>](#load-env-vars)\n",
    "3. [<span style=\"color:#4682B4\">Load Documentation from a Webpage</span>](#load-web-documentation)\n",
    "4. [<span style=\"color:#4682B4\">Text Splitting</span>](#text-splitting)\n",
    "5. [<span style=\"color:#4682B4\">Embedding and Vector Databases</span>](#embedding-vector-databases)\n",
    "6. [<span style=\"color:#4682B4\">Create a Retriever</span>](#create-retriever)\n",
    "7. [<span style=\"color:#4682B4\">Create Individual Tools</span>](#create-tools)\n",
    "    - [<span style=\"color:#4682B4\">7.1 LangSmith Tool</span>](#langsmith-tool)\n",
    "    - [<span style=\"color:#4682B4\">7.2 Wikipedia Tool</span>](#wikipedia-tool)\n",
    "    - [<span style=\"color:#4682B4\">7.3 Arxiv Tool</span>](#arxiv-tool)\n",
    "8. [<span style=\"color:#4682B4\">Collect Tools</span>](#collect-tools)\n",
    "9. [<span style=\"color:#4682B4\">Initialise the LLM</span>](#initialise-llm)\n",
    "10. [<span style=\"color:#4682B4\">Pull and Examine the Prompt</span>](#pull-prompt)\n",
    "11. [<span style=\"color:#4682B4\">Create the OpenAI Tools Agent</span>](#create-agent)\n",
    "12. [<span style=\"color:#4682B4\">Create the Agent Executor</span>](#create-agent-executor)\n",
    "13. [<span style=\"color:#4682B4\">Test the Agent Executor</span>](#test-agent-executor)\n",
    "    - [<span style=\"color:#4682B4\">13.1 LangSmith Example</span>](#langsmith-example)\n",
    "    - [<span style=\"color:#4682B4\">13.2 Wikipedia Example</span>](#wikipedia-example)\n",
    "    - [<span style=\"color:#4682B4\">13.3 Arxiv Example</span>](#arxiv-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**1. Import Libraries**</span> <a id=\"import-libraries\"></a>\n",
    "\n",
    "This section includes the necessary imports for building the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Import LangChain tools and utilities for querying Wikipedia and Arxiv\n",
    "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Import LangChain document loaders for different file types\n",
    "from langchain_community.document_loaders import WebBaseLoader          # For web scraping\n",
    "\n",
    "# Import libraries for environment variable management\n",
    "import os\n",
    "from dotenv import load_dotenv                                          # Securely load environment variables\n",
    "\n",
    "# Import OpenAI embeddings for vectorisation\n",
    "from langchain_openai import OpenAIEmbeddings  \n",
    "\n",
    "# Import LangChain utilities for splitting documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Import LangChain vector databases for storing embeddings\n",
    "from langchain_community.vectorstores import FAISS                      # Facebook's FAISS for similarity search\n",
    "\n",
    "# Import LangChain agents and tools for agent creation and execution\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "\n",
    "# Import OpenAI Chat models for LLM configuration\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Import LangChain hub for pulling prompts and configurations\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The module `from langchain_community.embeddings import OpenAIEmbeddings` is now **deprecated**. Instead, use `from langchain_openai import OpenAIEmbeddings`. To install the required package, run the following command in your terminal:\n",
    "`pip install -qU langchain-openai` \n",
    "\n",
    "**Note:**: The module `from langchain_community.llms import Ollama` is now **deprecated**. Instead, use `from langchain_ollama import OllamaLLM`. To install the required package, run the following command in your terminal:\n",
    "`pip install langchain-ollama`\n",
    "\n",
    "## <span style=\"color:#4682B4\">**2. Load Environment Variables**</span> <a id=\"load-env-vars\"></a>\n",
    "\n",
    "In this section, we securely load the OpenAI API key from a `.env` file. This ensures that sensitive information, such as API keys, is not hardcoded into the script. The `dotenv` library is used to manage environment variables effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the `.env` file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**3. Load Documentation from a Webpage**</span> <a id=\"load-web-documentation\"></a>\n",
    "\n",
    "In this section, we load documentation directly from the LangSmith webpage using the `WebBaseLoader`. \n",
    "\n",
    "This tool allows us to scrape and parse web content, converting it into a structured document format. The retrieved content will later be used for answering queries or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceQuick StartOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nWith LangSmith you can:\\n\\nTrace LLM Applications: Gain visibility into LLM calls and other parts of your application\\'s logic.\\nEvaluate Performance: Compare results across models, prompts, and architectures to identify what works best.\\nImprove Prompts: Quickly refine prompts to achieve more accurate and reliable results.\\n\\nLangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain\\'s open source frameworks langchain and langgraph, with no extra instrumentation needed.If you\\'re already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nLangSmith is a standalone platform that can be used on it\\'s own no matter how you\\'re creating your LLM applicatons.\\nIn this tutorial, we\\'ll walk you though logging your first trace in LangSmith using the LangSmith SDK and running an evaluation to measure the performance of your application. This example uses the OpenAI API, however you can use your provider of choice.\\n1. Install LangSmith\\u200b\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API key\\u200b\\nTo create an API key head to the Settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\\nShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key>export OPENAI_API_KEY=<your-openai-api-key>\\n4. Log your first trace\\u200b\\nWe provide multiple ways to log traces to LangSmith. Below, we\\'ll highlight\\nhow to use traceable(). See more on the Annotate code for tracing page.\\nPythonTypeScriptimport openaifrom langsmith import wrappers, traceable# Auto-trace LLM calls in-contextclient = wrappers.wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-4o-mini\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-4o-mini\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?\\nLearn more about tracing in the observability tutorials, conceptual guide and how-to guides.\\n5. View your trace\\u200b\\nBy default, the trace will be logged to the project with the name default. You should see the following sample output trace logged using the above code.\\n6. Run your first evaluation\\u200b\\nEvaluations help assess application performance by testing the application against a given set of inputs. Evaluations require a system to test, data to serve as test cases, and evaluators to grade the results.\\nHere we are running an evaluation against a sample dataset using a simple custom evaluator that checks if the real output exactly matches our gold-standard output.\\nPythonTypeScriptfrom langsmith import Client, traceableclient = Client()# Define dataset: these are your test casesdataset = client.create_dataset(    \"Sample Dataset\",    description=\"A sample dataset in LangSmith.\",)client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"response\": \"Welcome to LangSmith\"},        {\"response\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define an interface to your application (tracing optional)@traceabledef dummy_app(inputs: dict) -> dict:    return {\"response\": \"Welcome \" + inputs[\"postfix\"]}# Define your evaluator(s)def exact_match(outputs: dict, reference_outputs: dict) -> bool:    return outputs[\"response\"] == reference_outputs[\"response\"]# Run the evaluationexperiment_results = client.evaluate(    dummy_app, # Your AI system goes here    data=dataset, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={\"version\": \"1.0.0\", \"revision_id\": \"beta\"}, # Metadata about the experiment    max_concurrency=4,  # Add concurrency.)# Analyze the results via the UI or programmatically# If you have \\'pandas\\' installed you can view the results as a# pandas DataFrame by uncommenting below:# experiment_results.to_pandas()import { Client } from \"langsmith\";import { EvaluationResult, evaluate } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { response: \"Welcome to LangSmith\" },    { response: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluator(s)const exactMatch = async ({ outputs, referenceOutputs }: {  outputs?: Record<string, any>;  referenceOutputs?: Record<string, any>;}): Promise<EvaulationResult> => {  return {    key: \"exact_match\",    score: outputs?.response === referenceOutputs?.response,  };};// Run the evaluationconst experimentResults = await evaluate(  (inputs: { postfix: string }) => ({ response: `Welcome ${inputs.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: { version: \"1.0.0\", revision_id: \"beta\" },    maxConcurrency: 4,  });\\n\\nClick the link printed out by your evaluation run to access the LangSmith experiments UI,\\nand explore the results of your evaluation.\\nLearn more about evaluation in the tutorials, conceptual guide, and how-to guides.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextObservability tutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. View your trace6. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the WebBaseLoader to scrape and parse content from the specified webpage\n",
    "web_loader = WebBaseLoader(\n",
    "    \"https://docs.smith.langchain.com/\"\n",
    ")\n",
    "\n",
    "# Load the webpage's content into a document object\n",
    "langsmith_doc = web_loader.load()\n",
    "\n",
    "# Display the content of the loaded webpage to verify successful retrieval\n",
    "langsmith_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**4. Text Splitting**</span> <a id=\"text-splitting\"></a>\n",
    "\n",
    "In this section, we use the `RecursiveCharacterTextSplitter` from LangChain to divide the loaded document into smaller, overlapping chunks. This step is essential for processing large documents, ensuring each chunk remains within a manageable size for embedding and retrieval tasks.\n",
    "\n",
    "The parameters are:\n",
    "- **`chunk_size`**: Maximum size of each chunk.\n",
    "- **`chunk_overlap`**: Overlap between consecutive chunks to preserve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceQuick StartOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nWith LangSmith you can:\\n\\nTrace LLM Applications: Gain visibility into LLM calls and other parts of your application's logic.\\nEvaluate Performance: Compare results across models, prompts, and architectures to identify what works best.\\nImprove Prompts: Quickly refine prompts to achieve more accurate and reliable results.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"LangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain's open source frameworks langchain and langgraph, with no extra instrumentation needed.If you're already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nLangSmith is a standalone platform that can be used on it's own no matter how you're creating your LLM applicatons.\\nIn this tutorial, we'll walk you though logging your first trace in LangSmith using the LangSmith SDK and running an evaluation to measure the performance of your application. This example uses the OpenAI API, however you can use your provider of choice.\\n1. Install LangSmith\\u200b\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API key\\u200b\\nTo create an API key head to the Settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\\nShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key>export OPENAI_API_KEY=<your-openai-api-key>\\n4. Log your first trace\\u200b\\nWe provide multiple ways to log traces to LangSmith. Below, we'll highlight\\nhow to use traceable(). See more on the Annotate code for tracing page.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='4. Log your first trace\\u200b\\nWe provide multiple ways to log traces to LangSmith. Below, we\\'ll highlight\\nhow to use traceable(). See more on the Annotate code for tracing page.\\nPythonTypeScriptimport openaifrom langsmith import wrappers, traceable# Auto-trace LLM calls in-contextclient = wrappers.wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-4o-mini\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-4o-mini\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?\\nLearn more about tracing in the observability tutorials, conceptual guide and how-to guides.\\n5. View your trace\\u200b\\nBy default, the trace will be logged to the project with the name default. You should see the following sample output trace logged using the above code.\\n6. Run your first evaluation\\u200b\\nEvaluations help assess application performance by testing the application against a given set of inputs. Evaluations require a system to test, data to serve as test cases, and evaluators to grade the results.\\nHere we are running an evaluation against a sample dataset using a simple custom evaluator that checks if the real output exactly matches our gold-standard output.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='PythonTypeScriptfrom langsmith import Client, traceableclient = Client()# Define dataset: these are your test casesdataset = client.create_dataset(    \"Sample Dataset\",    description=\"A sample dataset in LangSmith.\",)client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"response\": \"Welcome to LangSmith\"},        {\"response\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define an interface to your application (tracing optional)@traceabledef dummy_app(inputs: dict) -> dict:    return {\"response\": \"Welcome \" + inputs[\"postfix\"]}# Define your evaluator(s)def exact_match(outputs: dict, reference_outputs: dict) -> bool:    return outputs[\"response\"] == reference_outputs[\"response\"]# Run the evaluationexperiment_results = client.evaluate(    dummy_app, # Your AI system goes here    data=dataset, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={\"version\": \"1.0.0\", \"revision_id\": \"beta\"}, # Metadata about the experiment    max_concurrency=4,  # Add concurrency.)# Analyze the results via the UI or programmatically# If you have \\'pandas\\' installed you can view the results as a# pandas DataFrame by uncommenting below:# experiment_results.to_pandas()import { Client } from \"langsmith\";import { EvaluationResult, evaluate } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { response: \"Welcome to LangSmith\" },    { response: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='\"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { response: \"Welcome to LangSmith\" },    { response: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluator(s)const exactMatch = async ({ outputs, referenceOutputs }: {  outputs?: Record<string, any>;  referenceOutputs?: Record<string, any>;}): Promise<EvaulationResult> => {  return {    key: \"exact_match\",    score: outputs?.response === referenceOutputs?.response,  };};// Run the evaluationconst experimentResults = await evaluate(  (inputs: { postfix: string }) => ({ response: `Welcome ${inputs.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: { version: \"1.0.0\", revision_id: \"beta\" },    maxConcurrency: 4,  });')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,                                    # Each chunk will have up to 2000 characters\n",
    "    chunk_overlap=200                                   # 200-character overlap between consecutive chunks\n",
    ")\n",
    "\n",
    "# Split the loaded web page into chunks\n",
    "web_doc = text_splitter.split_documents(langsmith_doc)\n",
    "\n",
    "# Display the first 5 chunks for inspection\n",
    "web_doc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**5. Embedding and Vector Databases**</span> <a id=\"embedding-vector-database\"></a>\n",
    "\n",
    "This section demonstrates how to embed document chunks into vector spaces using OpenAI embeddings and store them in a vector databases. The type of database used is Facebook's open-source library for fast vector similarity searches, **`FAISS`**.\n",
    "\n",
    "By embedding the first 30 document chunks, we prepare them for efficient similarity-based retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the first 20 document chunks into a FAISS vector database\n",
    "faiss_db = FAISS.from_documents(\n",
    "    web_doc[:30],  \n",
    "    OpenAIEmbeddings()  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**6. Create a Retriever Tool**</span> <a id=\"create-retriever\"></a>\n",
    "\n",
    "In this section, we convert the FAISS vector database into a retriever using the `as_retriever()` method. \n",
    "\n",
    "A **retriever** is a tool that fetches documents based on an unstructured query. Unlike a vector store, a retriever does not necessarily store documents‚Äîit only retrieves them. In this case, our FAISS vector store serves as the backbone for the retriever.\n",
    "\n",
    "For more details, refer to the [Langchain Retriever: documentation](https://python.langchain.com/docs/modules/data_connection/retrievers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000016E1B3BDDE0> search_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "# Convert the FAISS vector database into a retriever\n",
    "retriever = faiss_db.as_retriever()                 # The retriever fetches relevant documents based on an unstructured query\n",
    "\n",
    "# Print the retriever object to verify its creation\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**7. Create Individual Tools**</span> <a id=\"create-tools\"></a>\n",
    "\n",
    "In this section, we create the individual tools that will be used in the LangChain pipeline to process queries and retrieve information from specific sources.\n",
    "\n",
    "- <span style=\"color:#4682B4\">**7.1 LangSmith Tool**</span>: Retrieves information about LangSmith using a custom retriever.\n",
    "- <span style=\"color:#4682B4\">**7.2 Wikipedia Tool**</span>: Queries Wikipedia to fetch concise summaries.\n",
    "- <span style=\"color:#4682B4\">**7.3 Arxiv Tool**</span>: Queries the Arxiv database for academic papers and extracts relevant details.\n",
    "\n",
    "### <span style=\"color:#4682B4\">**7.1 LangSmith Tool**</span> <a id=\"langsmith-tool\"></a>\n",
    "\n",
    "The LangSmith tool uses a custom retriever to query the LangSmith documentation loaded earlier. It is specifically designed to handle questions about LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langsmith_search\n"
     ]
    }
   ],
   "source": [
    "# Create the LangSmith tool\n",
    "langsmith_tool = create_retriever_tool(\n",
    "    retriever,                                                                                              # Retriever: The retriever object for querying LangSmith documentation\n",
    "    \"langsmith_search\",                                                                                     # Name: A unique name for the tool\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool.\"    # Description: Instructions for when to use the tool\n",
    ")\n",
    "\n",
    "# Display the tool's name\n",
    "print(langsmith_tool.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4682B4\">**7.2 Wikipedia Tool**</span> <a id=\"wikipedia-tool\"></a>\n",
    "\n",
    "The Wikipedia tool queries Wikipedia for concise summaries. The tool is configured to return the top result with a maximum character limit for the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "# Create a Wikipedia API wrapper\n",
    "wiki_api_wrapper = WikipediaAPIWrapper(\n",
    "    top_k_results=1,                    # Return only the top result\n",
    "    doc_content_chars_max=200           # Limit content to 200 characters\n",
    ")\n",
    "\n",
    "# Create the Wikipedia query tool\n",
    "wiki_tool = WikipediaQueryRun(\n",
    "    api_wrapper=wiki_api_wrapper        # The API wrapper for querying Wikipedia\n",
    ")\n",
    "\n",
    "# Display the tool's name\n",
    "print(wiki_tool.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4682B4\">**7.3 Arxiv Tool**</span> <a id=\"arxiv-tool\"></a>\n",
    "\n",
    "The Arxiv tool queries the Arxiv database for academic articles. It is configured to return concise summaries of the most relevant paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "# Create an Arxiv API wrapper\n",
    "arxiv_wrapper_api = ArxivAPIWrapper(\n",
    "    top_k_results=1,                    # Return only the top result\n",
    "    doc_content_chars_max=200           # Limit content to 200 characters\n",
    ")\n",
    "\n",
    "# Create the Arxiv query tool\n",
    "arxiv_tool = ArxivQueryRun(\n",
    "    api_wrapper=arxiv_wrapper_api       # The API wrapper for querying Arxiv\n",
    ")\n",
    "\n",
    "# Display the tool's name\n",
    "print(arxiv_tool.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**8. Collect Tools**</span> <a id=\"collect-tools\"></a>\n",
    "\n",
    "In this section, we group the individual tools (LangSmith Tool, Wikipedia Tool, and Arxiv Tool) into a list for easy access. This list of tools will be passed to an agent in the subsequent steps to enable dynamic query handling across multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='langsmith_search' description='Search for information about LangSmith. For any questions about LangSmith, you must use this tool.' args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'> func=functools.partial(<function _get_relevant_documents at 0x0000016E79469090>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000016E1B3BDDE0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n') coroutine=functools.partial(<function _aget_relevant_documents at 0x0000016E797B57E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000016E1B3BDDE0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n') \n",
      "\n",
      "api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\The_R\\\\OneDrive\\\\Documents\\\\Portfolio\\\\AI\\\\LangChain\\\\6. RAG Q&A with Multiple Data Sources\\\\myenv\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200) \n",
      "\n",
      "api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the individual tools into a list for agent use\n",
    "agent_tools = [\n",
    "    langsmith_tool,                     # Retrieves information about LangSmith documentation\n",
    "    wiki_tool,                          # Queries Wikipedia for concise summaries\n",
    "    arxiv_tool                          # Retrieves information about academic papers from Arxiv\n",
    "]\n",
    "\n",
    "# Display the tools in the list to verify they have been added correctly\n",
    "for tool in agent_tools:\n",
    "    print(tool, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**9. Initialise the LLM**</span> <a id=\"initialise-llm\"></a>\n",
    "\n",
    "In this section, we initialise the `ChatOpenAI` large language model (LLM) using the GPT-3.5-turbo variant. This LLM will serve as the backbone for processing user queries and generating contextually relevant responses.\n",
    "\n",
    "- **`model`**: Specifies the LLM to use, in this case, `\"gpt-3.5-turbo-0125\"`.\n",
    "- **`temperature`**: Controls the randomness of the LLM's output. A value of `0` ensures deterministic and consistent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the ChatOpenAI large language model\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-3.5-turbo-0125\",\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**10. Pull and Examine the Prompt**</span> <a id=\"pull-prompt\"></a>\n",
    "\n",
    "In this section, we use the LangChain hub to pull a prebuilt prompt template for the OpenAI Functions Agent. The prompt provides a structure for interacting with tools and generating responses. \n",
    "\n",
    "After pulling the prompt, we examine its message structure to better understand its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\The_R\\OneDrive\\Documents\\Portfolio\\AI\\LangChain\\6. RAG Q&A with Multiple Data Sources\\myenv\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull a prebuilt prompt template from the LangChain hub\n",
    "prompt = hub.pull(\n",
    "    \"hwchase17/openai-functions-agent\"          # Identifier for the prompt template\n",
    ")\n",
    "\n",
    "# Display the message structure of the pulled prompt\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**11. Create the OpenAI Tools Agent**</span> <a id=\"create-agent\"></a>\n",
    "\n",
    "In this section, we create an OpenAI Tools Agent that combines the initialised LLM, the set of tools, and the custom prompt template. \n",
    "\n",
    "The agent orchestrates the interaction between the user query, tools, and the LLM, ensuring the query is processed efficiently to generate a relevant response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=RunnableAssign(mapper={\n",
      "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
      "}) middle=[ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000016E75BBF400>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000016E75BBF400>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')]), RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000016E1B5A2860>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000016E1B5BC9A0>, root_client=<openai.OpenAI object at 0x0000016E1B3BF7F0>, root_async_client=<openai.AsyncOpenAI object at 0x0000016E1B5A28C0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'Search for information about LangSmith. For any questions about LangSmith, you must use this tool.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])] last=OpenAIToolsAgentOutputParser()\n"
     ]
    }
   ],
   "source": [
    "# Create the OpenAI Tools Agent\n",
    "llm_agent = create_openai_tools_agent(\n",
    "    llm,\n",
    "    agent_tools,\n",
    "    prompt\n",
    ")\n",
    "\n",
    "# Display the created agent to verify its configuration\n",
    "print(llm_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**12. Create the Agent Executor**</span> <a id=\"create-agent-executor\"></a>\n",
    "\n",
    "In this section, we set up the `AgentExecutor`, which combines the OpenAI Tools Agent and the tools to execute queries dynamically. \n",
    "\n",
    "The `AgentExecutor` acts as the main interface, processing user inputs by coordinating the agent's logic with the defined tools. Setting `verbose=True` ensures detailed logging of the execution process for better debugging and transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose=True agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
      "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
      "})\n",
      "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000016E75BBF400>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000016E75BBF400>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
      "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000016E1B5A2860>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000016E1B5BC9A0>, root_client=<openai.OpenAI object at 0x0000016E1B3BF7F0>, root_async_client=<openai.AsyncOpenAI object at 0x0000016E1B5A28C0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'Search for information about LangSmith. For any questions about LangSmith, you must use this tool.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
      "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True) tools=[Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must use this tool.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000016E79469090>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000016E1B3BDDE0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000016E797B57E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000016E1B3BDDE0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n')), WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\The_R\\\\OneDrive\\\\Documents\\\\Portfolio\\\\AI\\\\LangChain\\\\6. RAG Q&A with Multiple Data Sources\\\\myenv\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200))]\n"
     ]
    }
   ],
   "source": [
    "# Create the Agent Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=llm_agent,                                   # The configured OpenAI Tools Agent\n",
    "    tools=agent_tools,                                 # List of tools to process queries\n",
    "    verbose=True                                       # Enable detailed execution logging\n",
    ")\n",
    "\n",
    "# Display the Agent Executor to verify its setup\n",
    "print(agent_executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4682B4\">**13. Test the Agent Executor**</span> <a id=\"test-agent-executor\"></a>\n",
    "\n",
    "In this section, we test the `AgentExecutor` with queries targeting the LangSmith documentation, Wikipedia, and Arxiv tools. Each query demonstrates the ability of the executor to dynamically interact with its tools and provide a relevant response.\n",
    "\n",
    "- <span style=\"color:#4682B4\">**13.1 LangSmith Example**</span>: Queries the LangSmith documentation for information about LangSmith.\n",
    "- <span style=\"color:#4682B4\">**13.2 Wikipedia Example**</span>: Uses the Wikipedia tool to retrieve a summary about Machine Learning.\n",
    "- <span style=\"color:#4682B4\">**13.3 Arxiv Example**</span>: Retrieves details about the academic paper with the identifier `1605.08386`.\n",
    "\n",
    "### <span style=\"color:#4682B4\">**13.1 LangSmith Example**</span> <a id=\"langsmith-example\"></a>\n",
    "\n",
    "This query fetches information about LangSmith using the LangSmith retriever tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'LangSmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceQuick StartOn this pageGet started with LangSmith\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "With LangSmith you can:\n",
      "\n",
      "Trace LLM Applications: Gain visibility into LLM calls and other parts of your application's logic.\n",
      "Evaluate Performance: Compare results across models, prompts, and architectures to identify what works best.\n",
      "Improve Prompts: Quickly refine prompts to achieve more accurate and reliable results.\n",
      "\n",
      "LangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain's open source frameworks langchain and langgraph, with no extra instrumentation needed.If you're already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "LangSmith is a standalone platform that can be used on it's own no matter how you're creating your LLM applicatons.\n",
      "In this tutorial, we'll walk you though logging your first trace in LangSmith using the LangSmith SDK and running an evaluation to measure the performance of your application. This example uses the OpenAI API, however you can use your provider of choice.\n",
      "1. Install LangSmith‚Äã\n",
      "PythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\n",
      "2. Create an API key‚Äã\n",
      "To create an API key head to the Settings page. Then click Create API Key.\n",
      "3. Set up your environment‚Äã\n",
      "Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key>export OPENAI_API_KEY=<your-openai-api-key>\n",
      "4. Log your first trace‚Äã\n",
      "We provide multiple ways to log traces to LangSmith. Below, we'll highlight\n",
      "how to use traceable(). See more on the Annotate code for tracing page.\n",
      "\n",
      "Click the link printed out by your evaluation run to access the LangSmith experiments UI,\n",
      "and explore the results of your evaluation.\n",
      "Learn more about evaluation in the tutorials, conceptual guide, and how-to guides.\n",
      "Was this page helpful?You can leave detailed feedback on GitHub.NextObservability tutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. View your trace6. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.\n",
      "\n",
      "4. Log your first trace‚Äã\n",
      "We provide multiple ways to log traces to LangSmith. Below, we'll highlight\n",
      "how to use traceable(). See more on the Annotate code for tracing page.\n",
      "PythonTypeScriptimport openaifrom langsmith import wrappers, traceable# Auto-trace LLM calls in-contextclient = wrappers.wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-4o-mini\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-4o-mini\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?\n",
      "Learn more about tracing in the observability tutorials, conceptual guide and how-to guides.\n",
      "5. View your trace‚Äã\n",
      "By default, the trace will be logged to the project with the name default. You should see the following sample output trace logged using the above code.\n",
      "6. Run your first evaluation‚Äã\n",
      "Evaluations help assess application performance by testing the application against a given set of inputs. Evaluations require a system to test, data to serve as test cases, and evaluators to grade the results.\n",
      "Here we are running an evaluation against a sample dataset using a simple custom evaluator that checks if the real output exactly matches our gold-standard output.\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform for building production-grade LLM (Large Language Model) applications. It allows users to closely monitor and evaluate their applications, enabling them to ship quickly and with confidence. Here are some key features of LangSmith:\n",
      "\n",
      "1. Trace LLM Applications: Gain visibility into LLM calls and other parts of your application's logic.\n",
      "2. Evaluate Performance: Compare results across models, prompts, and architectures to identify what works best.\n",
      "3. Improve Prompts: Quickly refine prompts to achieve more accurate and reliable results.\n",
      "\n",
      "LangSmith integrates seamlessly with LangChain's open-source frameworks langchain and langgraph, requiring no extra instrumentation. It can be used as a standalone platform regardless of how LLM applications are created.\n",
      "\n",
      "To get started with LangSmith, you can follow these steps:\n",
      "1. Install LangSmith using Python or TypeScript.\n",
      "2. Create an API key by heading to the Settings page and clicking on \"Create API Key.\"\n",
      "3. Set up your environment by exporting necessary variables like LANGCHAIN_TRACING_V2, LANGCHAIN_API_KEY, and OPENAI_API_KEY.\n",
      "4. Log your first trace using the provided methods.\n",
      "5. View your trace and explore the results in the LangSmith experiments UI.\n",
      "6. Run your first evaluation to assess application performance against a given set of inputs.\n",
      "\n",
      "For more detailed information and tutorials on LangSmith, you can visit the LangSmith homepage, blog, and documentation on their website.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about LangSmith.',\n",
       " 'output': 'LangSmith is a platform for building production-grade LLM (Large Language Model) applications. It allows users to closely monitor and evaluate their applications, enabling them to ship quickly and with confidence. Here are some key features of LangSmith:\\n\\n1. Trace LLM Applications: Gain visibility into LLM calls and other parts of your application\\'s logic.\\n2. Evaluate Performance: Compare results across models, prompts, and architectures to identify what works best.\\n3. Improve Prompts: Quickly refine prompts to achieve more accurate and reliable results.\\n\\nLangSmith integrates seamlessly with LangChain\\'s open-source frameworks langchain and langgraph, requiring no extra instrumentation. It can be used as a standalone platform regardless of how LLM applications are created.\\n\\nTo get started with LangSmith, you can follow these steps:\\n1. Install LangSmith using Python or TypeScript.\\n2. Create an API key by heading to the Settings page and clicking on \"Create API Key.\"\\n3. Set up your environment by exporting necessary variables like LANGCHAIN_TRACING_V2, LANGCHAIN_API_KEY, and OPENAI_API_KEY.\\n4. Log your first trace using the provided methods.\\n5. View your trace and explore the results in the LangSmith experiments UI.\\n6. Run your first evaluation to assess application performance against a given set of inputs.\\n\\nFor more detailed information and tutorials on LangSmith, you can visit the LangSmith homepage, blog, and documentation on their website.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the LangSmith tool via the agent executor\n",
    "langsmith_example = agent_executor.invoke(\n",
    "    {\"input\":\n",
    "     \"Tell me about LangSmith.\"\n",
    "     }\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "langsmith_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4682B4\">**13.2 Wikipedia Example**</span> <a id=\"wikipedia-example\"></a>\n",
    "\n",
    "This query retrieves a concise summary about Machine Learning using the Wikipedia tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Machine Learning'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Machine learning\n",
      "Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and gen\u001b[0m\u001b[32;1m\u001b[1;3mMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generate insights or predictions. It involves creating models that can automatically learn and improve from experience without being explicitly programmed. Machine learning algorithms are used in various applications such as image recognition, natural language processing, and recommendation systems.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Machine Learning.',\n",
       " 'output': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generate insights or predictions. It involves creating models that can automatically learn and improve from experience without being explicitly programmed. Machine learning algorithms are used in various applications such as image recognition, natural language processing, and recommendation systems.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the Wikipedia tool via the agent executor\n",
    "wiki_example = agent_executor.invoke(\n",
    "    {\"input\":\n",
    "     \"Tell me about Machine Learning.\"\n",
    "     }\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "wiki_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4682B4\">**13.3 Arxiv Example**</span> <a id=\"arxiv-example\"></a>\n",
    "\n",
    "This query retrieves details about the academic paper identified as `1605.08386` using the Arxiv tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1605.08386'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mPublished: 2016-05-26\n",
      "Title: Heat-bath random walks with Markov bases\n",
      "Authors: Caprice Stanley, Tobias Windisch\n",
      "Summary: Graphs on lattice points are studied whose edges come from a finite set of\n",
      "allo\u001b[0m\u001b[32;1m\u001b[1;3mThe paper with identifier 1605.08386 is titled \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch. The paper discusses the study of graphs on lattice points where the edges are derived from a finite set of allocations.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the paper 1605.08386 about?',\n",
       " 'output': 'The paper with identifier 1605.08386 is titled \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch. The paper discusses the study of graphs on lattice points where the edges are derived from a finite set of allocations.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the Arxiv tool via the agent executor\n",
    "arxiv_example = agent_executor.invoke(\n",
    "    {\"input\":\n",
    "     \"What is the paper 1605.08386 about?\"\n",
    "     }\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "arxiv_example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
